{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOyx3OQci7fHDsnAlizZihZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sahil02518/Major-Project/blob/main/Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVvFTEvIFJlV"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from wordcloud import WordCloud,STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from bs4 import BeautifulSoup\n",
        "import spacy\n",
        "import re,string,unicodedata\n",
        "from nltk.tokenize.toktok import ToktokTokenizer\n",
        "from nltk.stem import LancasterStemmer,WordNetLemmatizer\n",
        "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from textblob import TextBlob\n",
        "from textblob import Word\n",
        "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
        "\n",
        "import os\n",
        "print(os.listdir(\"../input\"))\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "imdb_data=pd.read_csv('../input/IMDB Dataset.csv')\n",
        "print(imdb_data.shape)\n",
        "imdb_data.head(10)\n",
        "imdb_data.describe()\n",
        "imdb_data['sentiment'].value_counts()\n",
        "train_reviews=imdb_data.review[:40000]\n",
        "train_sentiments=imdb_data.sentiment[:40000]\n",
        "test_reviews=imdb_data.review[40000:]\n",
        "test_sentiments=imdb_data.sentiment[40000:]\n",
        "print(train_reviews.shape,train_sentiments.shape)\n",
        "print(test_reviews.shape,test_sentiments.shape)\n",
        "tokenizer=ToktokTokenizer()\n",
        "stopword_list=nltk.corpus.stopwords.words('english')\n",
        "def strip_html(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "def remove_between_square_brackets(text):\n",
        "    return re.sub('\\[[^]]*\\]', '', text)\n",
        "def denoise_text(text):\n",
        "    text = strip_html(text)\n",
        "    text = remove_between_square_brackets(text)\n",
        "    return text\n",
        "imdb_data['review']=imdb_data['review'].apply(denoise_text)\n",
        "def remove_special_characters(text, remove_digits=True):\n",
        "    pattern=r'[^a-zA-z0-9\\s]'\n",
        "    text=re.sub(pattern,'',text)\n",
        "    return text\n",
        "imdb_data['review']=imdb_data['review'].apply(remove_special_characters)\n",
        "def simple_stemmer(text):\n",
        "    ps=nltk.porter.PorterStemmer()\n",
        "    text= ' '.join([ps.stem(word) for word in text.split()])\n",
        "    return text\n",
        "imdb_data['review']=imdb_data['review'].apply(simple_stemmer)\n",
        "stop=set(stopwords.words('english'))\n",
        "print(stop)\n",
        "def remove_stopwords(text, is_lower_case=False):\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [token.strip() for token in tokens]\n",
        "    if is_lower_case:\n",
        "        filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
        "    else:\n",
        "        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
        "    filtered_text = ' '.join(filtered_tokens)    \n",
        "    return filtered_text\n",
        "imdb_data['review']=imdb_data['review'].apply(remove_stopwords)\n",
        "norm_train_reviews=imdb_data.review[:40000]\n",
        "norm_train_reviews[0]\n",
        "norm_test_reviews=imdb_data.review[40000:]\n",
        "norm_test_reviews[45005]\n",
        "cv=CountVectorizer(min_df=0,max_df=1,binary=False,ngram_range=(1,3))\n",
        "cv_train_reviews=cv.fit_transform(norm_train_reviews)\n",
        "cv_test_reviews=cv.transform(norm_test_reviews)\n",
        "\n",
        "print('BOW_cv_train:',cv_train_reviews.shape)\n",
        "print('BOW_cv_test:',cv_test_reviews.shape)\n",
        "tv=TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,3))\n",
        "tv_train_reviews=tv.fit_transform(norm_train_reviews)\n",
        "tv_test_reviews=tv.transform(norm_test_reviews)\n",
        "print('Tfidf_train:',tv_train_reviews.shape)\n",
        "print('Tfidf_test:',tv_test_reviews.shape)\n",
        "lb=LabelBinarizer()\n",
        "sentiment_data=lb.fit_transform(imdb_data['sentiment'])\n",
        "print(sentiment_data.shape)\n",
        "train_sentiments=sentiment_data[:40000]\n",
        "test_sentiments=sentiment_data[40000:]\n",
        "print(train_sentiments)\n",
        "print(test_sentiments)\n",
        "lr=LogisticRegression(penalty='l2',max_iter=500,C=1,random_state=42)\n",
        "lr_bow=lr.fit(cv_train_reviews,train_sentiments)\n",
        "print(lr_bow)\n",
        "lr_tfidf=lr.fit(tv_train_reviews,train_sentiments)\n",
        "print(lr_tfidf)\n",
        "lr_bow_predict=lr.predict(cv_test_reviews)\n",
        "print(lr_bow_predict)\n",
        "lr_tfidf_predict=lr.predict(tv_test_reviews)\n",
        "print(lr_tfidf_predict)\n",
        "lr_bow_score=accuracy_score(test_sentiments,lr_bow_predict)\n",
        "print(\"lr_bow_score :\",lr_bow_score)\n",
        "lr_tfidf_score=accuracy_score(test_sentiments,lr_tfidf_predict)\n",
        "print(\"lr_tfidf_score :\",lr_tfidf_score)\n",
        "lr_bow_report=classification_report(test_sentiments,lr_bow_predict,target_names=['Positive','Negative'])\n",
        "print(lr_bow_report)\n",
        "lr_tfidf_report=classification_report(test_sentiments,lr_tfidf_predict,target_names=['Positive','Negative'])\n",
        "print(lr_tfidf_report)\n",
        "cm_bow=confusion_matrix(test_sentiments,lr_bow_predict,labels=[1,0])\n",
        "print(cm_bow)\n",
        "cm_tfidf=confusion_matrix(test_sentiments,lr_tfidf_predict,labels=[1,0])\n",
        "print(cm_tfidf)\n",
        "svm=SGDClassifier(loss='hinge',max_iter=500,random_state=42)\n",
        "svm_bow=svm.fit(cv_train_reviews,train_sentiments)\n",
        "print(svm_bow)\n",
        "svm_tfidf=svm.fit(tv_train_reviews,train_sentiments)\n",
        "print(svm_tfidf)\n",
        "svm_bow_predict=svm.predict(cv_test_reviews)\n",
        "print(svm_bow_predict)\n",
        "svm_tfidf_predict=svm.predict(tv_test_reviews)\n",
        "print(svm_tfidf_predict)\n",
        "svm_bow_score=accuracy_score(test_sentiments,svm_bow_predict)\n",
        "print(\"svm_bow_score :\",svm_bow_score)\n",
        "svm_tfidf_score=accuracy_score(test_sentiments,svm_tfidf_predict)\n",
        "print(\"svm_tfidf_score :\",svm_tfidf_score)\n",
        "svm_bow_report=classification_report(test_sentiments,svm_bow_predict,target_names=['Positive','Negative'])\n",
        "print(svm_bow_report)\n",
        "svm_tfidf_report=classification_report(test_sentiments,svm_tfidf_predict,target_names=['Positive','Negative'])\n",
        "print(svm_tfidf_report)\n",
        "cm_bow=confusion_matrix(test_sentiments,svm_bow_predict,labels=[1,0])\n",
        "print(cm_bow)\n",
        "cm_tfidf=confusion_matrix(test_sentiments,svm_tfidf_predict,labels=[1,0])\n",
        "print(cm_tfidf)\n",
        "mnb=MultinomialNB()\n",
        "mnb_bow=mnb.fit(cv_train_reviews,train_sentiments)\n",
        "print(mnb_bow)\n",
        "mnb_tfidf=mnb.fit(tv_train_reviews,train_sentiments)\n",
        "print(mnb_tfidf)\n",
        "mnb_bow_predict=mnb.predict(cv_test_reviews)\n",
        "print(mnb_bow_predict)\n",
        "mnb_tfidf_predict=mnb.predict(tv_test_reviews)\n",
        "print(mnb_tfidf_predict)\n",
        "mnb_bow_score=accuracy_score(test_sentiments,mnb_bow_predict)\n",
        "print(\"mnb_bow_score :\",mnb_bow_score)\n",
        "mnb_tfidf_score=accuracy_score(test_sentiments,mnb_tfidf_predict)\n",
        "print(\"mnb_tfidf_score :\",mnb_tfidf_score)\n",
        "mnb_bow_report=classification_report(test_sentiments,mnb_bow_predict,target_names=['Positive','Negative'])\n",
        "print(mnb_bow_report)\n",
        "mnb_tfidf_report=classification_report(test_sentiments,mnb_tfidf_predict,target_names=['Positive','Negative'])\n",
        "print(mnb_tfidf_report)\n",
        "cm_bow=confusion_matrix(test_sentiments,mnb_bow_predict,labels=[1,0])\n",
        "print(cm_bow)\n",
        "cm_tfidf=confusion_matrix(test_sentiments,mnb_tfidf_predict,labels=[1,0])\n",
        "print(cm_tfidf)\n",
        "plt.figure(figsize=(10,10))\n",
        "positive_text=norm_train_reviews[1]\n",
        "WC=WordCloud(width=1000,height=500,max_words=500,min_font_size=5)\n",
        "positive_words=WC.generate(positive_text)\n",
        "plt.imshow(positive_words,interpolation='bilinear')\n",
        "plt.show\n",
        "plt.figure(figsize=(10,10))\n",
        "negative_text=norm_train_reviews[8]\n",
        "WC=WordCloud(width=1000,height=500,max_words=500,min_font_size=5)\n",
        "negative_words=WC.generate(negative_text)\n",
        "plt.imshow(negative_words,interpolation='bilinear')\n",
        "plt.show"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}